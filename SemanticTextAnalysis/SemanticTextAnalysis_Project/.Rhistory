#install.packages("XML")
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain))
# export
writeCorpus(ovid)
# little details
print(ovid)
# more details
inspect(ovid[1:2])
# more details
meta(ovid)
meta(ovid[1:2])
meta(ovid[[2])
meta(ovid[[2]])
meta(ovid[[2]], "id");
identical(ovid[[2]], ovid[["ovid_2.txt"]])
identical(ovid[[2]], ovid[["ovidasd_2.txt"]])
writeLines("hejsa")
writeLines(as.character(ovid))
writeLines(as.character(ovid[[2]]))
writeLines(as.character(ovid[2]))
as.character(ovid[[2]])
?lapply
lapply(ovid[1:2], as.character)
writeLines(lapply(ovid[1:2], as.character))
lapply(ovid[1:2], as.character)
reuters <- tm_map(reuters, stripWhitespace)
reuters <- tm_map(reuters, content_transformer(tolower))
?gsub
reuters <- tm_map(reuters, removeWords, stopwords("english"))
tm_map(reuters, stemDocument)
install.packages("SnowballC")
tm_map(reuters, stemDocument)
install.packages('SnowballC');
tm_map(reuters, stemDocument)
reuters_stemmed = tm_map(reuters, stemDocument)
writeLines(as.character(ovid[[2]]))
writeLines(as.character(reuters))
writeLines(as.character(reuters[[2]]))
writeLines(as.character(reuters_stemmed[[2]]))
idx <- meta(reuters, "id") == '237' & meta(reuters, "heading") == 'INDONESIA SEEN AT CROSSROADS OVER ECONOMIC CHANGE'
reuters[idx]
idx <- meta(reuters, "id") == '237' | meta(reuters, "id") == '238'
reuters[idx]
idx <- meta(reuters, "id") == '237' | meta(reuters, "id") == '238'
reuters[idx]
idx <- meta(reuters, "id") == '1237' | meta(reuters, "id") == '238'
reuters[idx]
idx <- meta(reuters, "id") == '237' | meta(reuters, "id") == '230'
reuters[idx]
meta(ovid[[2]]);
meta(reuters, "id")
idx <- meta(reuters, "id") == '237' | meta(reuters, "id") == '704'
reuters[idx]
meta(reuters)
meta(reuters[[2]])
?DublinCore
DublinCore(crude[[1]], "Creator") <- "Ano Nymous"
DublinCore(ovid[[1]], "Creator") = "Ano Nymous"
DublinCore(ovid[[1]], "Creator")
meta(ovid[[1]])
meta(ovid[[1]], tag = "test", type = "corpus") <- "test meta"
meta(ovid[[1]])
meta(ovid[[1]], type = "corpus")
meta(ovid[[1]])
$test
meta(crude, type = "corpus")
$test
attr(,"class")
meta(ovid[[1]], "foo") <- letters[1:20]
meta(ovid[[1]])
rm(list=ls(all=TRUE))
setwd("C:/Users/Wikzo/Documents/Hagenberg_2015/SemanticTextAnalysis/SemanticTextAnalysis_Project")
wd
getwd()
library(RCurl)
library(XML)
library(stringr)
library(tm)
library(SnowballC)
library(RWeka)
library(RTextTools)
library(topicmodels)
install.packages("stringr")
install.packages(c("RCurl", "RWeka", "RTextTools", "topicmodels"))
library(RCurl)
library(XML)
library(stringr)
library(tm)
library(SnowballC)
library(RWeka)
library(RTextTools)
library(topicmodels)
library(RCurl)
library(XML)
library(stringr)
library(tm)
library(SnowballC)
library(RWeka)
library(RTextTools)
library(topicmodels)
RCurl
?RCurl
?XML
XMLSource
cls
library(RCurl)
library(XML)
library(stringr)
library(tm)
library(SnowballC)
library(RWeka)
library(RTextTools)
library(topicmodels)
?xpathSApply
all_links <- character()
new_results <- 'government/announcements?keywords=&announcement_type_option=press-releases&topics[]=all&departments[]=all&world_locations[]=all&from_date=&to_date=01%2F07%2F2010'
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
while(length(new_results) > 0){
new_results <- str_c("https://www.gov.uk/", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//li[@id]//a", xmlGetAttr, "href"))
new_results <- xpathSApply(results_tree, "//nav[@id='show-more-documents']//li[@class='next']//a", xmlGetAttr, "href")
}
all_links[1]
length(all_links)
# Download all press releases
for(i in 1:length(all_links)){
url <- str_c("https://www.gov.uk", all_links[i])
tmp <- getURL(url, cainfo = signatures)
write(tmp, str_c("Press_Releases/", i, ".html"))
}
length(list.files("Press_Releases"))
list.files("Press_Releases")[1:3]
head(list.files("Press_Releases"))
?head
?head
tmp <- readLines("Press_Releases/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release <- xpathSApply(tmp, "//div[@class='block-4']", xmlValue)
n
organisation <- xpathSApply(tmp, "//a[@class='organisation-link']", xmlValue)
organisation
publication <- xpathSApply(tmp, "//time[@class='date']", xmlValue)
publication
release_corpus <- Corpus(VectorSource(release))
meta(release_corpus[[1]], "organisation") <- organisation[1]
meta(release_corpus[[1]], "publication") <- publication[1]
meta(release_corpus[[1]])
n <- 1
for(i in 2:length(list.files("Press_Releases/"))){
tmp <- readLines(str_c("Press_Releases/", i, ".html"))
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release <- xpathSApply(tmp, "//div[@class='block-4']", xmlValue)
organisation <- xpathSApply(tmp, "//a[@class='organisation-link']", xmlValue)
publication <- xpathSApply(tmp, "//time[@class='date']", xmlValue)
if(length(release) != 0){
n <- n + 1
tmp_corpus <- Corpus(VectorSource(release))
release_corpus <- c(release_corpus, tmp_corpus)
meta(release_corpus[[n]], "organisation") <- organisation[1]
meta(release_corpus[[n]], "publication") <- publication[1]
}
}
release_corpus
meta(release_corpus)
meta_organisation <- meta(release_corpus, type = "local", tag = "organisation")
meta_publication <- meta(release_corpus, type = "local", tag = "publication")
meta_data <- data.frame(
organisation = unlist(meta_organisation),
publication = unlist(meta_publication)
)
table(as.character(meta_data[, "organisation"]))
table(as.character(meta_data[, "publication"]))
table(as.character(meta_data[, "organisation"]))
release_corpus <- release_corpus[
meta(release_corpus, tag = "organisation") == "Department for Business, Innovation & Skills" |
meta(release_corpus, tag = "organisation") == "Department for Communities and Local Government" |
meta(release_corpus, tag = "organisation") == "Department for Environment, Food & Rural Affairs" |
meta(release_corpus, tag = "organisation") == "Foreign & Commonwealth Office" |
meta(release_corpus, tag = "organisation") == "Ministry of Defence" |
meta(release_corpus, tag = "organisation") == "Wales Office"
]
release_corpus
tm_filter(release_corpus, FUN = function(x) any(grep("Afghanistan", content(x))))
tdm <- TermDocumentMatrix(release_corpus)
tdm
release_corpus <- tm_map(release_corpus, removeNumbers)
release_corpus <- tm_map(
release_corpus,
content_transformer(
function(x, pattern){
gsub(
pattern = "[[:punct:]]",
replacement = " ",
x
)
}
)
)
length(stopwords("en"))
stopwords("en")[1:10]
release_corpus <- tm_map(release_corpus, removeWords, words = stopwords("en"))
release_corpus <- tm_map(release_corpus, content_transformer(tolower))
release_corpus <- tm_map(release_corpus, stemDocument)
release_corpus
head(release_corpus)
tdm <- TermDocumentMatrix(release_corpus)
tdm
tdm <- removeSparseTerms(tdm, 1-(10/length(release_corpus)))
tdm
BigramTokenizer <- function(x){
NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
tdm_bigram <- TermDocumentMatrix(release_corpus, control = list(tokenize = BigramTokenizer))
tdm_bigram
findAssocs(tdm, "nuclear", .7)
findAssocs(tdm, "and", .7)
findAssocs(tdm, "US", .7)
findAssocs(tdm, "war", .7)
findAssocs(tdm, "bush", .7)
findAssocs(tdm, "nuclear", .7)
findAssocs(tdm, "office", .7)
findAssocs(tdm, "office", .1)
findAssocs(tdm, "office", .9)
findAssocs(tdm, "nuclear", .7)
?findAssocs
?findAssocs
findAssocs(tdm, "nuclear")
findAssocs(tdm, "nuclear", 1)
all_links <- character()
new_results <- 'pressreleases_index.php'
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
while(length(new_results) > 0){
new_results <- str_c("http://gamasutra.com/", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//li[@id]//a", xmlGetAttr, "href"))
new_results <- xpathSApply(results_tree, "//nav[@id='show-more-documents']//li[@class='next']//a", xmlGetAttr, "href")
}
all_links[1]
length(all_links)
new_results <- str_c("http://gamasutra.com/", new_results)
new_results
results <- getURL(new_results, cainfo = signatures)
results
?str_c
new_results = "?page=10";
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
new_results
capture.output(cat('?page=',pageNumber))
pageNumber = 1;
capture.output(cat('?page=',pageNumber))
paste(c("?page=", pageNumber), collapse = "")
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
new_results
pageNumber = pageNumber +1;
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
new_results
all_links <- character()
pageNumber = 1;
new_results = paste(c("?page=", pageNumber), collapse = "")
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
while(length(new_results) < 10){
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//li/a", xmlGetAttr, "href"))
pageNumber = pageNumber +1;
#new_results <- xpathSApply(results_tree, "//nav[@id='show-more-documents']//[@class='pageNav']//a", xmlGetAttr, "href")
}
all_links <- character()
pageNumber = 1;
new_results = paste(c("?page=", pageNumber), collapse = "")
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
while(length(new_results) < 2){
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//li/a", xmlGetAttr, "href"))
pageNumber = pageNumber +1;
#new_results <- xpathSApply(results_tree, "//nav[@id='show-more-documents']//[@class='pageNav']//a", xmlGetAttr, "href")
}
length(new_results
length(new_results)
all_links <- character()
pageNumber = 1;
new_results = paste(c("?page=", pageNumber), collapse = "")
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
while(length(new_results) < 2){
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//li/a", xmlGetAttr, "href"))
pageNumber = pageNumber +1;
#new_results <- xpathSApply(results_tree, "//nav[@id='show-more-documents']//[@class='pageNav']//a", xmlGetAttr, "href")
}
library(RCurl)
library(XML)
library(stringr)
library(tm)
library(SnowballC)
library(RWeka)
library(RTextTools)
library(topicmodels)
setwd("C:/Users/Wikzo/Documents/Hagenberg_2015/SemanticTextAnalysis/SemanticTextAnalysis_Project")
all_links <- character()
pageNumber = 1;
new_results = paste(c("?page=", pageNumber), collapse = "")
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
while(length(new_results) < 2){
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//li/a", xmlGetAttr, "href"))
pageNumber = pageNumber +1;
#new_results <- xpathSApply(results_tree, "//nav[@id='show-more-documents']//[@class='pageNav']//a", xmlGetAttr, "href")
}
setwd("C:/Users/Wikzo/Documents/Hagenberg_2015/SemanticTextAnalysis/SemanticTextAnalysis_Project")
library(RCurl)
library(XML)
library(stringr)
library(tm)
library(SnowballC)
library(RWeka)
library(RTextTools)
library(topicmodels)
all_links <- character()
pageNumber = 1;
new_results = paste(c("?page=", pageNumber), collapse = "")
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "[@class='leftnav bottom2']//li/a", xmlGetAttr, "href"))
all_links <- c(all_links, xpathSApply(results_tree, "\\[@class='leftnav bottom2']//li/a", xmlGetAttr, "href"))
all_links <- c(all_links, xpathSApply(results_tree, "//[@class='leftnav bottom2']//li/a", xmlGetAttr, "href"))
all_links <- c(all_links, xpathSApply(results_tree, "//li/a", xmlGetAttr, "href"))
all_links[1]
all_links
all_links <- c(all_links, xpathSApply(results_tree, "/li/a", xmlGetAttr, "href"))
all_links
all_links <- c(all_links, xpathSApply(results_tree, "//li[@class='story_title']/a", xmlGetAttr, "href"))
all_links
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//li[@class='story_title']/a", xmlGetAttr, "href"))
all_links
length(all_links)
all_links <- c(all_links, xpathSApply(results_tree, "//li[@class='story_title']//a", xmlGetAttr, "href"))
length(all_links)
all_links <- c(all_links, xpathSApply(results_tree, "//li[@class='NewsContent']/a", xmlGetAttr, "href"))
pageNumber = pageNumber +1;
length(all_links)
pageNumber = 1;
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//li[@class='story_title']/a", xmlGetAttr, "href"))
length(all_links)
all_links <- c(all_links, xpathSApply(results_tree, "//span[@class='story_title']/a", xmlGetAttr, "href"))
length(all_links)
all_links <- c(all_links, xpathSApply(results_tree, "//td[@class='NewsContent']//span[@class='story_title']/a", xmlGetAttr, "href"))
all_links <- c(all_links, xpathSApply(results_tree, "//td[@class='NewsContent']/span[@class='story_title']/a", xmlGetAttr, "href"))
length(all_links)
all_links <- character()
all_links <- c(all_links, xpathSApply(results_tree, "//td[@class='NewsContent']/span[@class='story_title']/a", xmlGetAttr, "href"))
length(all_links)
all_links <- character()
pageNumber = 1;
new_results = paste(c("?page=", pageNumber), collapse = "")
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
while(length(new_results) < 2){
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//td[@class='NewsContent']/span[@class='story_title']/a", xmlGetAttr, "href"))
pageNumber = pageNumber +1;
#new_results <- xpathSApply(results_tree, "//nav[@id='show-more-documents']//[@class='pageNav']//a", xmlGetAttr, "href")
}
setwd("C:/Users/Wikzo/Documents/Hagenberg_2015/SemanticTextAnalysis/SemanticTextAnalysis_Project")
getwd()
# load packages
library(RCurl)
library(XML)
library(stringr)
library(tm)
library(SnowballC)
library(RWeka)
library(RTextTools)
library(topicmodels)
all_links <- character()
pageNumber = 1;
new_results = paste(c("?page=", pageNumber), collapse = "")
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
while(pageNumber < 2){
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//td[@class='NewsContent']/span[@class='story_title']/a", xmlGetAttr, "href"))
pageNumber = pageNumber +1;
#new_results <- xpathSApply(results_tree, "//nav[@id='show-more-documents']//[@class='pageNav']//a", xmlGetAttr, "href")
}
all_links[1]
length(all_links)
# Download all press releases (748 articles)
for(i in 1:length(all_links)){
url <- str_c("http://gamasutra.com/", all_links[i])
tmp <- getURL(url, cainfo = signatures)
write(tmp, str_c("Gamasutra/", i, ".html"))
}
all_links[1]
for(i in 1:length(all_links)){
url <- all_links[i]
tmp <- getURL(url, cainfo = signatures)
write(tmp, str_c("Gamasutra/", i, ".html"))
}
tmp
write(tmp, str_c(url, i, ".html"))
# Download all press releases (748 articles)
for(i in 1:length(all_links)){
url <- all_links[i]
tmp <- getURL(url, cainfo = signatures)
write(tmp, str_c(url, i, ".html"))
}
url <- str_c("http://gamasutra.com/", all_links[i])
url
url = all_links[i];
url
length(list.files("Gamasutra"))
all_links <- character()
pageNumber = 1;
new_results = paste(c("?page=", pageNumber), collapse = "")
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
while(pageNumber < 3){
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//td[@class='NewsContent']/span[@class='story_title']/a", xmlGetAttr, "href"))
pageNumber = pageNumber +1;
#new_results <- xpathSApply(results_tree, "//nav[@id='show-more-documents']//[@class='pageNav']//a", xmlGetAttr, "href")
}
all_links[1]
length(all_links)
for(i in 1:length(all_links)){
#url <- str_c("http://gamasutra.com/", all_links[i])
url = all_links[i];
tmp <- getURL(url, cainfo = signatures)
write(tmp, str_c("Gamasutra/", i, ".html"))
}
length(list.files("Gamasutra"))
list.files("Gamasutra")[1:3]
head(list.files("Gamasutra")) #first or last part of object
tmp <- readLines("Press_Releases/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
tmp <- readLines("Gamasutra/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release <- xpathSApply(tmp, "//td[@class='newsText']", xmlValue)
release <- xpathSApply(tmp, "//td[@class='newsText']//<p>", xmlValue)
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
release
keywords = xpathSApply(tmp, "//meta/@keywords']", xmlValue)
keywords = doc["//meta/@keywords"]
keywords = tmp["//meta/@keywords"]
keywords
keywords = results_tree["//meta/@keywords"]
keywords
keywords = results_tree[1]["//meta/@keywords"]
keywords = xpathSApply(tmp, "//meta", xmlValue)
keywords
keywords = xpathSApply(tmp, "/meta", xmlValue)
keywords
keywords = xpathSApply(tmp, "/meta[@name='keywords']", xmlValue)
keywords
keywords = xpathSApply(tmp, "//meta[@name='keywords']", xmlValue)
keywords
tmp <- readLines("Gamasutra/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
keywords = xpathSApply(tmp, "//meta[@name='keywords']", xmlValue)
keywords
keywords = tmp["//meta/@name"]
keywords
keywords = tmp["//meta/@name='keywords'"]
keywords
keywords = tmp["//meta/@name]
keywords
keywords = tmp["//meta/@name]
keywords = tmp["//meta/@name"]
keywords
content = tmp["//meta/@content"]
cbind(names,content)
cbind(names,content)
