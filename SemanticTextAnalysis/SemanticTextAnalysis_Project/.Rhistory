n <- n + 1
tmp_corpus <- Corpus(VectorSource(release))
release_corpus <- c(release_corpus, tmp_corpus)
meta(release_corpus[[n]], "section") <- section
meta(release_corpus[[n]], "publication_date") <- date[1]
}
}
release_corpus
# Inspect meta data
meta_section <- meta(release_corpus, type = "local", tag = "section")
meta_publication_date <- meta(release_corpus, type = "local", tag = "publication_date")
meta_data <- data.frame(
section = unlist(meta_section),
publication_date = unlist(meta_publication_date)
)
table(as.character(meta_data[, "section"]))
table(as.character(meta_data[, "publication_date"]))
head(meta_data["section"])
head(meta_data["publication_date"])
######
# Filtering the corpus (only those with enought sources; for supervised learning)
release_corpus <- release_corpus[
meta(release_corpus, tag = "section") == "Accessories" | #22
meta(release_corpus, tag = "section") == "Android OS" | #10
#meta(release_corpus, tag = "section") == "Android TV" | #6
meta(release_corpus, tag = "section") == " Android Wear " | #11
#meta(release_corpus, tag = "section") == "APK Teardown " | #6
meta(release_corpus, tag = "section") == "Applications" | #145
meta(release_corpus, tag = "section") == "AT&T" | #11
#meta(release_corpus, tag = "section") == "Chromecast" | #5
meta(release_corpus, tag = "section") == "Deals" | #37
#meta(release_corpus, tag = "section") == "Development" | #6
meta(release_corpus, tag = "section") == "Device Updates" | #10
meta(release_corpus, tag = "section") == "Games" | #27
meta(release_corpus, tag = "section") == "Google" | #29
#meta(release_corpus, tag = "section") == "Leaks" | #6
meta(release_corpus, tag = "section") == "Marshmallow 6.0" | #20
meta(release_corpus, tag = "section") == "News" #54
]
release_corpus
tm_filter(release_corpus, FUN = function(x) any(grep("Google", content(x))))
### 10.2.2 Building a term-document matrix
### --------------------------------------------------------------
# sparse: matrix contains zero's
tdm <- TermDocumentMatrix(release_corpus)
tdm
### 10.2.3 Data cleansing
### --------------------------------------------------------------
# Remove numbers
release_corpus <- tm_map(release_corpus, removeNumbers)
# Remove punctuation
release_corpus <- tm_map(
release_corpus,
content_transformer(
function(x, pattern){
gsub(
pattern = "[[:punct:]]",
replacement = " ",
x
)
}
)
)
# Remove stopwords
length(stopwords("en"))
stopwords("en")[1:10]
release_corpus <- tm_map(release_corpus, removeWords, words = stopwords("en"))
# Convert to lower case
release_corpus <- tm_map(release_corpus, content_transformer(tolower))
# Stem documents
release_corpus <- tm_map(release_corpus, stemDocument)
### 10.2.4 Sparsity and n-grams
### --------------------------------------------------------------
tdm <- TermDocumentMatrix(release_corpus)
tdm
# Remove sparse terms
tdm <- removeSparseTerms(tdm, 1-(10/length(release_corpus)))
tdm
# Bigrams (not needed at the moment...)
BigramTokenizer <- function(x){
NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
tdm_bigram <- TermDocumentMatrix(release_corpus, control = list(tokenize = BigramTokenizer))
tdm_bigram
# Find associations
findAssocs(tdm, "Google", .7)
### 10.3 Supervised Learning Techniques
### --------------------------------------------------------------
### 10.3.5 Application: Governemnt press releases
### --------------------------------------------------------------
dtm <- DocumentTermMatrix(release_corpus)
dtm <- removeSparseTerms(dtm, 1-(10/length(release_corpus)))
dtm
# Labels
org_labels <- unlist(meta(release_corpus, "section"))
org_labels[1:3]
training_size = (length(release_corpus) / 100) * 80;
test_size = training_size + 1;#(length(release_corpus) / 100) * 20;
# Create container
N <- length(org_labels)
container <- create_container(
dtm,
labels = org_labels,
trainSize = 1:training_size,
testSize = test_size:N,
virgin = F
)
slotNames(container)
# Train models
svm_model <- train_model(container, "SVM")
tree_model <- train_model(container, "TREE")
maxent_model <- train_model(container, "MAXENT")
# Classify models
svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)
head(svm_out)
head(tree_out)
head(maxent_out)
# Construct data frame with correct labels
labels_out <- data.frame(
correct_label = org_labels[test_size:N],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F
)
## SVM performance
table(labels_out[,1] == labels_out[,2])
## Random forest performance
table(labels_out[,1] == labels_out[,3])
## Maximum entropy performance
table(labels_out[,1] == labels_out[,4])
prop.table(table(labels_out[,1] == labels_out[,4]))
### 10.4 Unsupervised Learning Techniques
### --------------------------------------------------------------
### 10.4.2 Application: Government press releases
### --------------------------------------------------------------
# Hierarchical clustering
# Create shortened corpus
short_corpus <- release_corpus[c(
which(
meta(
release_corpus, tag = "section"
) == "Accessories"
)[1:20],
which(
meta(
release_corpus, tag = "section"
) == "Applications"
)[1:20],
which(
meta(
release_corpus, tag = "section"
) == "Marshmallow 6.0"
)[1:20],
which(
meta(
release_corpus, tag = "section"
) == "AT&T"
)[1:18] # <--- this number CANNOT be lower than the actual occurences of articles (otherwise NA error)
)]
table(unlist(meta(short_corpus, "section")))
# Create shortened Document-Term-Matrix
short_dtm <- DocumentTermMatrix(short_corpus)
short_dtm <- removeSparseTerms(short_dtm, 1-(5/length(short_corpus)))
rownames(short_dtm) <- c(rep("Accessories", 20), rep("Applications", 20), rep("Marshmallow", 20), rep("AT&T", 18))
# Create dendrogram
dist_dtm <- dist(short_dtm)
out <- hclust(dist_dtm, method = "ward.D")
plot(out)
# draw the clusters: http://www.instantr.com/2013/02/12/performing-a-cluster-analysis-in-r/
rect.hclust(out, 5)
# Unsupervised classification
# use length of org_labels
# length(table(org_labels))
lda_out <- LDA(dtm, 10)
posterior_lda <- posterior(lda_out)
lda_topics <- data.frame(t(posterior_lda$topics))
## Setting up matrix for mean probabilities
mean_topic_matrix <- matrix(
NA,
nrow = 10,
ncol = 10,
dimnames = list(
names(table(org_labels)),
str_c("Topic_", 1:10)
)
)
for(i in 1:6){
mean_topic_matrix[i,] <- apply(lda_topics[, which(org_labels == rownames(mean_topic_matrix)[i])], 1, mean)
}
round(mean_topic_matrix, 2)
terms(lda_out, 10)
ctm_out <- CTM(dtm, 11)
terms(ctm_out, 10)
par(mfrow = c(2,3), cex.main = .8, pty = "s", mar = c(5, 5, 1, 1))
for(topic in 1:2){
for(orga in names(table(org_labels))){
tmp.data <- ctm_topics[topic, org_labels == orga]
plot(
1:ncol(tmp.data),
sort(as.numeric(tmp.data)),
type = "l",
ylim = c(0, 1),
xlab = "Press releases",
ylab = str_c("Posterior probability, topic ", topic),
main = str_replace(orga, "Department for", "")
)
}
}
par(mfrow = c(2,3), cex.main = .8, pty = "s", mar = c(5, 5, 1, 1))
for(topic in 1:2){
for(orga in names(table(org_labels))){
tmp.data <- ctm_topics[topic, org_labels == orga]
plot(
1:ncol(tmp.data),
sort(as.numeric(tmp.data)),
type = "l",
ylim = c(0, 1),
xlab = "Press releases",
ylab = str_c("Posterior probability, topic ", topic),
main = str_replace(orga, "Department for", "")
)
}
}
ctm_topics
posterior_ctm <- posterior(ctm_out)
ctm_topics <- data.frame(t(posterior_ctm$topics))
par(mfrow = c(2,3), cex.main = .8, pty = "s", mar = c(5, 5, 1, 1))
for(topic in 1:2){
for(orga in names(table(org_labels))){
tmp.data <- ctm_topics[topic, org_labels == orga]
plot(
1:ncol(tmp.data),
sort(as.numeric(tmp.data)),
type = "l",
ylim = c(0, 1),
xlab = "Press releases",
ylab = str_c("Posterior probability, topic ", topic),
main = str_replace(orga, "Department for", "")
)
}
}
library(streamR)
library(lubridate)
library(stringr)
library(plyr)
library(twitteR)
library(RCurl)
library(ROAuth)
requestURL <-  "https://api.twitter.com/oauth/request_token"
accessURL =    "https://api.twitter.com/oauth/access_token"
authURL =      "https://api.twitter.com/oauth/authorize"
consumerKey =   "g8V1ywv6SEJphCGseyWagmeoy"
consumerSecret = "7zX8CHnstU9gUBFC3qRVClsKG9x8MqdQbuReMb3q1AR4VeAkS1"
twitCred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=requestURL,
accessURL=accessURL,
authURL=authURL)
twitCred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
5393020
# Filter the Twitter stream
filterStream("tweets.json", track = c("iwatch", "iphone"),
timeout = 300, oauth = twitCred)
requestURL <-  "https://api.twitter.com/oauth/request_token"
accessURL =    "https://api.twitter.com/oauth/access_token"
authURL =      "https://api.twitter.com/oauth/authorize"
consumerKey =   "g8V1ywv6SEJphCGseyWagmeoy"
consumerSecret = "7zX8CHnstU9gUBFC3qRVClsKG9x8MqdQbuReMb3q1AR4VeAkS1"
twitCred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=requestURL,
accessURL=accessURL,
authURL=authURL)
twitCred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
0501528
filterStream("tweets.json", track = c("iwatch", "iphone"),
timeout = 300, oauth = twitCred)
tweets <- parseTweets("tweets.json", simplify = TRUE)
Sys.setlocale("LC_TIME", "en_US.UTF-8")
tweets$created_at[1]
tweets$time <- as.POSIXct(tweets$created_at, tz = "UTC", format = "%a %b %d %H
:%M:%S %z %Y")
tweets$round_minute <- round_date(tweets$time, unit = "minute")
tweets
tweets
tweets <- parseTweets("tweets.json", simplify = TRUE)
Sys.setlocale("LC_TIME", "en_US.UTF-8")
tweets$created_at[1]
tweets$created_at[1]
tweets$time <- as.POSIXct(tweets$created_at, tz = "UTC", format = "%a %b %d %H
:%M:%S %z %Y")
tweets$round_minute <- round_date(tweets$time, unit = "minute")
tweets$round_minute
tweets$time
Sys.setlocale("LC_TIME", "en_US.UTF-8")
tweets$text
tweets$lang
tweets$truncated
tweets$lang
tweets$source
tweets$text
tweets$time
tweets <- parseTweets("tweets.json", simplify = TRUE)
tweets$text
tweets
tweets$created_at[1]
tweets$created_at
tweets$created_at[1]
tweets$created_at
[1]
tweets$time <- as.POSIXct(tweets$created_at, tz = "UTC", format = "%a %b %d %H
:%M:%S %z %Y")
tweets$time
tweets
tweets$verified
tweets$time
tweets$time
tweets$text
# Parse values
tweets$time <- as.POSIXct(tweets$created_at, tz = "UTC", format = "%a %b %d %H
:%M:%S %z %Y")
Sys.setlocale("LC_TIME", "en_US.UTF-8")
tweets$time <- as.POSIXct(tweets$created_at, tz = "UTC", format = "%M")
tweets$time
as.POSIXct(tweets$created_at, tz = "UTC", format = "%M")
tweets$time <- as.POSIXct(tweets$created_at, tz = "UTC", format = "%M")
tweets$time
tweets$time <- as.POSIXct(tweets$created_at, tz = "UTC", format = "%a %b %d %H
:%M:%S %z %Y")
tweets$time
tweets$time <- as.POSIXct(tweets$created_at, format = "%M")
tweets$time
tweets$time
tweets <- parseTweets("tweets.json", simplify = TRUE)
tweets$time
tweets$time_mine <- as.POSIXct(tweets$created_at, tz = "UTC", format = "%M")
tweets$time_mine
tweets$time
tweets <- parseTweets("tweets.json", simplify = TRUE)
tweets$time
tweets$created_at
my_Time <- as.POSIXct(tweets$created_at, tz = "UTC", format = "%M")
my_Time
as.POSIXct(tweets$created_at, tz = "UTC", format = "%M")
tweets$created_at
as.POSIXct(tweets$created_at, tz = "UTC", format = "%a %b %d %H
:%M:%S %z %Y")
Sys.setlocale("LC_TIME", "English")
my_Time <- as.POSIXct(tweets$created_at, tz = "UTC", format = "%M")
my_Time
as.POSIXct(tweets$created_at, tz = "UTC", format = "%a %b %d %H
:%M:%S %z %Y")
as.POSIXct(tweets$created_at, tz = "UTC", format = "%H
:")
tweets$time <- as.POSIXct(tweets$created_at, tz = "UTC", format = "%a : %M")
tweets$time
tweets <- parseTweets("tweets.json", simplify = TRUE)
as.POSIXct(tweets$created_at, tz = "UTC", format = "%a : %M")
as.POSIXct(tweets$created_at, tz = "UTC", format = "%a %b %d %H
:%M:%S %z %Y")
tweets$round_minute <- round_date(tweets$time, unit = "minute")
tweets$time
tweets$round_minute <- round_date(tweets$time, unit = "minute")
Sys.setlocale("LC_TIME", "English")
tweets <- parseTweets("tweets.json", simplify = TRUE)
Sys.setlocale("LC_TIME", "English")
as.POSIXct(tweets$created_at, tz = "UTC", format = "%a %b %d %H
:%M:%S %z %Y")
tweets$time <- as.POSIXct(tweets$created_at, tz = "UTC", format = "%a %b %d %H:%M:%S %z %Y")
tweets$round_minute <- round_date(tweets$time, unit = "minute")
tweets$round_minute
plot(tweets$round_minute)
strftime(tweets$time, format="%H:%M:%S")
strftime(tweets$time, format="%M")
strftime(tweets$time, format="%H:%M:%S")
strftime(tweets$time, format="%M")
myMinutes = strftime(tweets$time, format="%M")
plot(myMinutes)
plot(myMinutes,myMinutes)
length(tweets)
tweets
length(tweets)
length(tweets$time)
length(tweets$text)
length(tweets$time)
length(myMinutes)
indexList = list(rep(list(c()), times=10));
indexList
indexList = list((c());
indexList = list(1);
indexList
indexList = list(10);
list[9] = 23;
indexList = list(0,0,0,0,0,0,0,0,0,0);
list[9] = 23;
indexList[9] = 23;
length(myMinutes)
myMinutes[2866]
maxTime = myMinutes[1]
myMinutes[1]
indexList[1]++;
indexList[1] = indexList[1] + 1;
indexList[1]
indexList[1] + 1
myMinutes[1]
length(myMinutes)
length(2866)
myMinutes[2866]
indexList[2] = 10;
indexList[2] = indexList[1];
indexList[2] = indexList[1];
indexList[2] = 10;
indexList[3] = 10;
indexList[4] = indexList[3] + indexList[4];
indexList[4] = indexList[3] + 1;
indexList[4] = indexList[[3]] + 1;
index = 0;
for (i in 1:length(tweets$time))
{
if (myMinutes[i] != currentMinute)
{
index = index + 1;
currentMinute = myMinutes[i];
}
indexList[index] = indexList[[index]] + 1;
}
currentMinute = 0;
indexList = list(0,0,0,0,0,0,0,0,0,0);
index = 0;
for (i in 1:length(tweets$time))
{
if (myMinutes[i] != currentMinute)
{
index = index + 1;
currentMinute = myMinutes[i];
}
indexList[index] = indexList[[index]] + 1;
}
currentMinute = 0;
indexList = list(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
index = 0;
for (i in 1:length(tweets$time))
{
if (myMinutes[i] != currentMinute)
{
index = index + 1;
currentMinute = myMinutes[i];
}
indexList[index] = indexList[[index]] + 1;
}
currentMinute = 0;
indexList = list(0,0,0,0,0,0,0,0,0,0,0);
index = 0;
for (i in 1:length(tweets$time))
{
if (myMinutes[i] != currentMinute)
{
index = index + 1;
currentMinute = myMinutes[i];
}
indexList[index] = indexList[[index]] + 1;
}
plot(myMinutes,indexList[index])
index
plot(indexList[index])
plot(c(1:11),indexList[index])
plot(c(1:11),unlist(indexList[index])
plot(c(1:11),unlist(indexList[index]))
plot(c(1:11),unlist(indexList[index]))
plot(unlist(indexList[index])
indexList[index]
unlist(indexList[index])
plot(c(1:11),unlist(indexList))
plot(c(1:11),unlist(indexList), type="n", main=heading)
plot(c(1:11),unlist(indexList),type="o", col="blue")
plot(0:23, d, type='b', axes=FALSE)
axis(side=1, at=c(0:23))
axis(side=2, at=seq(0, 600, by=100))
box()
axis(side=1, at=c(0:11))
plot(c(1:11),unlist(indexList),type="o", col="blue")
axis(side=1, at=c(0:11))
library(streamR)
library(lubridate)
library(stringr)
library(plyr)
library(twitteR)
library(RCurl)
library(ROAuth)
axis(side=1, at=c(0:11))
requestURL <-  "https://api.twitter.com/oauth/request_token"
accessURL =    "https://api.twitter.com/oauth/access_token"
authURL =      "https://api.twitter.com/oauth/authorize"
consumerKey =   "g8V1ywv6SEJphCGseyWagmeoy"
consumerSecret = "7zX8CHnstU9gUBFC3qRVClsKG9x8MqdQbuReMb3q1AR4VeAkS1"
twitCred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=requestURL,
accessURL=accessURL,
authURL=authURL)
twitCred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
twitCred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=requestURL,
accessURL=accessURL,
authURL=authURL)
twitCred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
filterStream("tweets.json", track = c("iwatch", "iphone"),
timeout = 150, oauth = twitCred)
