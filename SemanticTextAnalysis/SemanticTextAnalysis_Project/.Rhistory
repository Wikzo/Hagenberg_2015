}
setwd("C:/Users/Wikzo/Documents/Hagenberg_2015/SemanticTextAnalysis/SemanticTextAnalysis_Project")
getwd()
# load packages
library(RCurl)
library(XML)
library(stringr)
library(tm)
library(SnowballC)
library(RWeka)
library(RTextTools)
library(topicmodels)
all_links <- character()
pageNumber = 1;
new_results = paste(c("?page=", pageNumber), collapse = "")
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
while(pageNumber < 2){
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//td[@class='NewsContent']/span[@class='story_title']/a", xmlGetAttr, "href"))
pageNumber = pageNumber +1;
#new_results <- xpathSApply(results_tree, "//nav[@id='show-more-documents']//[@class='pageNav']//a", xmlGetAttr, "href")
}
all_links[1]
length(all_links)
# Download all press releases (748 articles)
for(i in 1:length(all_links)){
url <- str_c("http://gamasutra.com/", all_links[i])
tmp <- getURL(url, cainfo = signatures)
write(tmp, str_c("Gamasutra/", i, ".html"))
}
all_links[1]
for(i in 1:length(all_links)){
url <- all_links[i]
tmp <- getURL(url, cainfo = signatures)
write(tmp, str_c("Gamasutra/", i, ".html"))
}
tmp
write(tmp, str_c(url, i, ".html"))
# Download all press releases (748 articles)
for(i in 1:length(all_links)){
url <- all_links[i]
tmp <- getURL(url, cainfo = signatures)
write(tmp, str_c(url, i, ".html"))
}
url <- str_c("http://gamasutra.com/", all_links[i])
url
url = all_links[i];
url
length(list.files("Gamasutra"))
all_links <- character()
pageNumber = 1;
new_results = paste(c("?page=", pageNumber), collapse = "")
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
while(pageNumber < 3){
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//td[@class='NewsContent']/span[@class='story_title']/a", xmlGetAttr, "href"))
pageNumber = pageNumber +1;
#new_results <- xpathSApply(results_tree, "//nav[@id='show-more-documents']//[@class='pageNav']//a", xmlGetAttr, "href")
}
all_links[1]
length(all_links)
for(i in 1:length(all_links)){
#url <- str_c("http://gamasutra.com/", all_links[i])
url = all_links[i];
tmp <- getURL(url, cainfo = signatures)
write(tmp, str_c("Gamasutra/", i, ".html"))
}
length(list.files("Gamasutra"))
list.files("Gamasutra")[1:3]
head(list.files("Gamasutra")) #first or last part of object
tmp <- readLines("Press_Releases/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
tmp <- readLines("Gamasutra/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release <- xpathSApply(tmp, "//td[@class='newsText']", xmlValue)
release <- xpathSApply(tmp, "//td[@class='newsText']//<p>", xmlValue)
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
release
keywords = xpathSApply(tmp, "//meta/@keywords']", xmlValue)
keywords = doc["//meta/@keywords"]
keywords = tmp["//meta/@keywords"]
keywords
keywords = results_tree["//meta/@keywords"]
keywords
keywords = results_tree[1]["//meta/@keywords"]
keywords = xpathSApply(tmp, "//meta", xmlValue)
keywords
keywords = xpathSApply(tmp, "/meta", xmlValue)
keywords
keywords = xpathSApply(tmp, "/meta[@name='keywords']", xmlValue)
keywords
keywords = xpathSApply(tmp, "//meta[@name='keywords']", xmlValue)
keywords
tmp <- readLines("Gamasutra/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
keywords = xpathSApply(tmp, "//meta[@name='keywords']", xmlValue)
keywords
keywords = tmp["//meta/@name"]
keywords
keywords = tmp["//meta/@name='keywords'"]
keywords
keywords = tmp["//meta/@name]
keywords
keywords = tmp["//meta/@name]
keywords = tmp["//meta/@name"]
keywords
content = tmp["//meta/@content"]
cbind(names,content)
cbind(names,content)
library(RCurl)
library(XML)
library(stringr)
library(tm)
library(SnowballC)
library(RWeka)
library(RTextTools)
library(topicmodels)
keywords = tmp["//meta/@name"]
tmp <- readLines("Gamasutra/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
keywords = tmp["//meta/@name"]
content = tmp["//meta/@content"]
cbind(names,content)
keywords = keywords[3]
keywords
keywords = tmp["//meta/@name"]
keywords
content = tmp["//meta/@content"]
cbind(keywords,content)
k = cbind(keywords,content)
k[3]
keys = k[3]
keys
keys = content[3]
keys
date <- xpathSApply(tmp, "//td[@class='newsDate']", xmlValue)
date
release_corpus <- Corpus(VectorSource(release))
release_corpus
head(release_corpus)
meta(release_corpus[[1]])
release
release <- xpathSApply(tmp, "//div[@class='block-4']", xmlValue)
release
tmp <- readLines("Press_Releases/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release <- xpathSApply(tmp, "//div[@class='block-4']", xmlValue)
release
tmp <- readLines("Gamasutra/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
str_c(release ,collapse=',')
a = str_c(release ,collapse=',')
tmp <- readLines("Gamasutra/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
release_corpus <- Corpus(VectorSource(release))
head(release_corpus)
release_corpus <- Corpus(VectorSource(release))
head(release_corpus)
# Setting the meta information
meta(release_corpus[[1]], "keywords") <- keywords
meta(release_corpus[[1]], "publication_date") <- date
meta(release_corpus[[1]])
keywords
content[3]
keys = content[3]
keys
meta(release_corpus[[1]], "keywords") <- keys
meta(release_corpus[[1]])
n <- 1
## make corpus of all articles
for(i in 2:length(list.files("Gamasutra/"))){
tmp <- readLines(str_c("Gamasutra/", i, ".html"))
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
# article content
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
# meta data: keywords
content = tmp["//meta/@content"]
keys = content[3]
# meta data: publication date
date <- xpathSApply(tmp, "//td[@class='newsDate']", xmlValue)
if(length(release) != 0){
n <- n + 1
tmp_corpus <- Corpus(VectorSource(release))
release_corpus <- c(release_corpus, tmp_corpus)
meta(release_corpus[[1]], "keywords") <- keys
meta(release_corpus[[1]], "publication_date") <- date
}
}
release_corpus
meta_organisation <- meta(release_corpus, type = "local", tag = "keywords")
meta_keywords <- meta(release_corpus, type = "local", tag = "keywords")
meta_publication_date <- meta(release_corpus, type = "local", tag = "publication_date")
meta_data <- data.frame(
organisation = unlist(meta_keywords),
publication = unlist(meta_publication_date)
)
table(as.character(meta_data[, "keywords"]))
meta_keywords <- meta(release_corpus, type = "local", tag = "keywords")
meta_publication_date <- meta(release_corpus, type = "local", tag = "publication_date")
meta_data <- data.frame(
organisation = unlist(meta_keywords),
publication = unlist(meta_publication_date)
)
table(as.character(meta_data[, "keywords"]))
release_corpus
meta(release_corpus[[1]])
meta(release_corpus[[2]])
all_links <- character()
pageNumber = 1;
new_results = paste(c("?page=", pageNumber), collapse = "")
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")
while(pageNumber < 3){
new_results = paste(c("?page=", pageNumber), collapse = "")
new_results <- str_c("http://gamasutra.com/pressreleases_index.php", new_results)
results <- getURL(new_results, cainfo = signatures)
results_tree <- htmlParse(results)
all_links <- c(all_links, xpathSApply(results_tree, "//td[@class='NewsContent']/span[@class='story_title']/a", xmlGetAttr, "href"))
pageNumber = pageNumber +1;
#new_results <- xpathSApply(results_tree, "//nav[@id='show-more-documents']//[@class='pageNav']//a", xmlGetAttr, "href")
}
all_links[1]
length(all_links)
for(i in 1:length(all_links)){
#url <- str_c("http://gamasutra.com/", all_links[i])
url = all_links[i];
tmp <- getURL(url, cainfo = signatures)
write(tmp, str_c("Gamasutra/", i, ".html"))
}
length(list.files("Gamasutra"))
list.files("Gamasutra")[1:3]
head(list.files("Gamasutra")) #first or last part of object
tmp <- readLines("Gamasutra/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
keywords = tmp["//meta/@name"]
content = tmp["//meta/@content"]
keys = content[3]
date <- xpathSApply(tmp, "//td[@class='newsDate']", xmlValue)
release_corpus <- Corpus(VectorSource(release))
head(release_corpus)
meta(release_corpus[[1]], "keywords") <- keys
meta(release_corpus[[1]], "publication_date") <- date
meta(release_corpus[[1]])
meta(release_corpus[[2]])
n <- 1
## make corpus of all articles
for(i in 2:length(list.files("Gamasutra/"))){
tmp <- readLines(str_c("Gamasutra/", i, ".html"))
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
# article content
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
# meta data: keywords
content = tmp["//meta/@content"]
keys = content[3]
# meta data: publication date
date <- xpathSApply(tmp, "//td[@class='newsDate']", xmlValue)
if(length(release) != 0){
n <- n + 1
tmp_corpus <- Corpus(VectorSource(release))
release_corpus <- c(release_corpus, tmp_corpus)
meta(release_corpus[[1]], "keywords") <- keys
meta(release_corpus[[1]], "publication_date") <- date
}
}
head(release_corpus)
release_corpus
meta(release_corpus[[2]])
meta(release_corpus[[3]])
tmp_corpus
tmp_corpus <- Corpus(VectorSource(release))
tmp_corpus
VectorSource(release)
n <- 1
## make corpus of all articles
for(i in 2:length(list.files("Gamasutra/"))){
tmp <- readLines(str_c("Gamasutra/", i, ".html"))
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
# article content
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
# meta data: keywords
content = tmp["//meta/@content"]
keys = content[3]
# meta data: publication date
date <- xpathSApply(tmp, "//td[@class='newsDate']", xmlValue)
if(length(release) != 0){
n <- n + 1
tmp_corpus <- Corpus(VectorSource(release))
release_corpus <- c(release_corpus, tmp_corpus)
meta(release_corpus[[n]], "keywords") <- keys
meta(release_corpus[[n]], "publication_date") <- date
}
}
release_corpus
head(release_corpus)
release_corpus
release_corpus <- Corpus(VectorSource(release))
head(release_corpus)
n <- 1
## make corpus of all articles
for(i in 2:length(list.files("Gamasutra/"))){
tmp <- readLines(str_c("Gamasutra/", i, ".html"))
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
# article content
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
# meta data: keywords
content = tmp["//meta/@content"]
keys = content[3]
# meta data: publication date
date <- xpathSApply(tmp, "//td[@class='newsDate']", xmlValue)
if(length(release) != 0){
n <- n + 1
tmp_corpus <- Corpus(VectorSource(release))
release_corpus <- c(release_corpus, tmp_corpus)
meta(release_corpus[[n]], "keywords") <- keys
meta(release_corpus[[n]], "publication_date") <- date
}
}
meta_keywords <- meta(release_corpus, type = "local", tag = "keywords")
meta_publication_date <- meta(release_corpus, type = "local", tag = "publication_date")
meta_data <- data.frame(
organisation = unlist(meta_keywords),
publication = unlist(meta_publication_date)
)
table(as.character(meta_data[, "keywords"]))
meta_keywords <- meta(release_corpus, type = "local", tag = "keywords")
meta_publication_date <- meta(release_corpus, type = "local", tag = "publication_date")
meta_data <- data.frame(
keywords = unlist(meta_keywords),
publication_date = unlist(meta_publication_date)
)
table(as.character(meta_data[, "keywords"]))
table(as.character(meta_data[, "publication_date"]))
meta_keywords
meta(release_corpus[[n]], "keywords")
meta(release_corpus[[5]], "keywords")
tmp <- readLines(str_c("Gamasutra/", i, ".html"))
tmp
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
tmp
## make corpus of all articles
for(i in 2:length(list.files("Gamasutra/"))){
tmp <- readLines(str_c("Gamasutra/", i, ".html"))
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
# article content
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
# meta data: keywords
content = tmp["//meta/@content"]
keys = content[3]
# meta data: publication date
date <- xpathSApply(tmp, "//td[@class='newsDate']", xmlValue)
if(length(release) != 0){
n <- n + 1
tmp_corpus <- Corpus(VectorSource(release))
release_corpus <- c(release_corpus, tmp_corpus)
meta(release_corpus[[n]], "keywords") <- keys
meta(release_corpus[[n]], "publication_date") <- date
}
}
n <- 1
n <- 1
## make corpus of all articles
for(i in 2:length(list.files("Gamasutra/"))){
tmp <- readLines(str_c("Gamasutra/", i, ".html"))
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
# article content
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
# meta data: keywords
content = tmp["//meta/@content"]
keys = content[3]
# meta data: publication date
date <- xpathSApply(tmp, "//td[@class='newsDate']", xmlValue)
if(length(release) != 0){
n <- n + 1
tmp_corpus <- Corpus(VectorSource(release))
release_corpus <- c(release_corpus, tmp_corpus)
meta(release_corpus[[n]], "keywords") <- keys
meta(release_corpus[[n]], "publication_date") <- date
}
}
n
release_corpus
tmp <- readLines("Gamasutra/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release = NULL
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
# Get meta information (organisation and date of publication)
## https://stackoverflow.com/questions/22342501/how-to-get-information-within-meta-name-tag-in-html-using-htmlparse-and-xpa
# keywords
keywords = tmp["//meta/@name"]
content = tmp["//meta/@content"]
#cbind(keywords,content)
keys = content[3]
# publication date
date <- xpathSApply(tmp, "//td[@class='newsDate']", xmlValue)
# Create a corpus from a vector
release_corpus = NULL;
release_corpus <- Corpus(VectorSource(release))
head(release_corpus)
# Setting the meta information
meta(release_corpus[[1]], "keywords") <- keys
meta(release_corpus[[1]], "publication_date") <- date
meta(release_corpus[[1]])
n <- 1
## make corpus of all articles
for(i in 2:length(list.files("Gamasutra/"))){
tmp <- readLines(str_c("Gamasutra/", i, ".html"))
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
# article content
release <- xpathSApply(tmp, "//td[@class='newsText']/p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
# meta data: keywords
content = tmp["//meta/@content"]
keys = content[3]
# meta data: publication date
date <- xpathSApply(tmp, "//td[@class='newsDate']", xmlValue)
if(length(release) != 0){
n <- n + 1
tmp_corpus <- Corpus(VectorSource(release))
release_corpus <- c(release_corpus, tmp_corpus)
meta(release_corpus[[n]], "keywords") <- keys
meta(release_corpus[[n]], "publication_date") <- date
}
}
release_corpus
meta_keywords <- meta(release_corpus, type = "local", tag = "keywords")
meta_keywords
meta_publication_date <- meta(release_corpus, type = "local", tag = "publication_date")
meta_publication_date
meta_data <- data.frame(
keywords = unlist(meta_keywords),
publication_date = unlist(meta_publication_date)
)
table(as.character(meta_data[, "publication_date"]))
table(as.character(meta_data[, "keywords"]))
tm_filter(release_corpus, FUN = function(x) any(grep("Afghanistan", content(x))))
tm_filter(release_corpus, FUN = function(x) any(grep("Nintendo", content(x))))
tdm <- TermDocumentMatrix(release_corpus)
tdm
release_corpus <- tm_map(release_corpus, removeNumbers)
# Remove punctuation
release_corpus <- tm_map(
release_corpus,
content_transformer(
function(x, pattern){
gsub(
pattern = "[[:punct:]]",
replacement = " ",
x
)
}
)
)
length(stopwords("en"))
stopwords("en")[1:10]
release_corpus <- tm_map(release_corpus, removeWords, words = stopwords("en"))
# Convert to lower case
release_corpus <- tm_map(release_corpus, content_transformer(tolower))
# Stem documents
release_corpus <- tm_map(release_corpus, stemDocument)
tdm <- TermDocumentMatrix(release_corpus)
tdm
# Remove sparse terms
tdm <- removeSparseTerms(tdm, 1-(10/length(release_corpus)))
tdm
# Bigrams (not needed at the moment...)
BigramTokenizer <- function(x){
NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
tdm_bigram <- TermDocumentMatrix(release_corpus, control = list(tokenize = BigramTokenizer))
tdm_bigram
findAssocs(tdm, "video", .7)
findAssocs(tdm, "sony", .7)
findAssocs(tdm, "epic", .7)
findAssocs(tdm, "metal", .7)
findAssocs(tdm, "and", .7)
findAssocs(tdm, "metal", .1)
findAssocs(tdm, "metal", .9)
