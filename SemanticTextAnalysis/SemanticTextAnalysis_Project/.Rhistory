n <- 1
## make corpus of all articles
for(i in 2:length(list.files("androidpolice/")))
#for(i in 1:20)
{
tmp <- readLines(str_c("androidpolice/", i, ".html"))
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
# article content
release <- xpathSApply(tmp, "//div[@class='post-content']//p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
# meta data: section
content = tmp["//meta/@content"]
# get all meta data, but mark everything except 'property' as NULL
properties = xpathSApply(tmp, "//meta", xmlGetAttr, "property")
# find the meta data containing the 'section' property
sectionIndex = 0;
dateIndex = 0;
for (j in 1:length(properties))
{
if (grepl("article:section",properties[j]) == TRUE)
sectionIndex = j;
if (grepl("article:published_time",properties[j]) == TRUE)
dateIndex = j;
}
# set the section and publication date
section = content[sectionIndex];
date = content[dateIndex];
# make from list to string character
section = paste(c(section), collapse='')
date = paste(c(date), collapse='')
date = strsplit(date, "T")[[1]] #only take the first string (without clock time)
#print(section)
if(length(release) != 0)
{
n <- n + 1
tmp_corpus <- Corpus(VectorSource(release))
release_corpus <- c(release_corpus, tmp_corpus)
meta(release_corpus[[n]], "section") <- section
meta(release_corpus[[n]], "publication_date") <- date
}
}
tmp <- readLines("androidpolice/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release = NULL
release <- xpathSApply(tmp, "//div[@class='post-content']//p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
### GET META DATA:
## https://stackoverflow.com/questions/22342501/how-to-get-information-within-meta-name-tag-in-html-using-htmlparse-and-xpa
# article section (meta data)
# <meta property="article:section" content="Accessories"/>
# meta data: section
content = tmp["//meta/@content"]
# get all meta data, but mark everything except 'property' as NULL
properties = xpathSApply(tmp, "//meta", xmlGetAttr, "property")
# find the meta data containing the 'section' property
sectionIndex = 0;
dateIndex = 0;
for (j in 1:length(properties))
{
if (grepl("article:section",properties[j]) == TRUE)
sectionIndex = j;
if (grepl("article:published_time",properties[j]) == TRUE)
dateIndex = j;
}
# set the section and publication date
section = content[sectionIndex];
date = content[dateIndex];
# make from list to string character
section = paste(c(section), collapse='')
date = paste(c(date), collapse='')
#date = strsplit(date, "T")[[1]] only take the first string (without clock time)
# Create a corpus from a vector
release_corpus = NULL;
release_corpus <- Corpus(VectorSource(release))
head(release_corpus)
# Setting the meta information
meta(release_corpus[[1]], "section") <- section
meta(release_corpus[[1]], "publication_date") <- date
meta(release_corpus[[1]])
n <- 1
## make corpus of all articles
for(i in 2:length(list.files("androidpolice/")))
#for(i in 1:20)
{
tmp <- readLines(str_c("androidpolice/", i, ".html"))
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
# article content
release <- xpathSApply(tmp, "//div[@class='post-content']//p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
# meta data: section
content = tmp["//meta/@content"]
# get all meta data, but mark everything except 'property' as NULL
properties = xpathSApply(tmp, "//meta", xmlGetAttr, "property")
# find the meta data containing the 'section' property
sectionIndex = 0;
dateIndex = 0;
for (j in 1:length(properties))
{
if (grepl("article:section",properties[j]) == TRUE)
sectionIndex = j;
if (grepl("article:published_time",properties[j]) == TRUE)
dateIndex = j;
}
# set the section and publication date
section = content[sectionIndex];
date = content[dateIndex];
# make from list to string character
section = paste(c(section), collapse='')
date = paste(c(date), collapse='')
date = strsplit(date, "T")[[1]] #only take the first string (without clock time)
#print(section)
if(length(release) != 0)
{
n <- n + 1
tmp_corpus <- Corpus(VectorSource(release))
release_corpus <- c(release_corpus, tmp_corpus)
meta(release_corpus[[n]], "section") <- section
meta(release_corpus[[n]], "publication_date") <- date
}
}
release_corpus
propertyA = tmp["//meta/@property"]
propertyA
propertyA[1]
propertyA[[1]]
meta_section <- meta(release_corpus, type = "local", tag = "section")
meta_publication_date <- meta(release_corpus, type = "local", tag = "publication_date")
meta_data <- data.frame(
section = unlist(meta_section),
publication_date = unlist(meta_publication_date)
)
table(as.character(meta_data[, "section"]))
table(as.character(meta_data[, "publication_date"]))
date
meta_section <- meta(release_corpus, type = "local", tag = "section")
meta_publication_date <- meta(release_corpus, type = "local", tag = "publication_date")
meta_publication_date
length(meta_publication_date)
length(meta_section)
meta_data <- data.frame(
section = unlist(meta_section),
publication_date = unlist(meta_publication_date)
)
meta_data <- data.frame(
section = unlist(meta_section),
publication_date = unlist(meta_publication_date[[1]])
)
table(as.character(meta_data[, "section"]))
table(as.character(meta_data[, "publication_date"]))
meta_data <- data.frame(
section = unlist(meta_section),
publication_date = unlist(meta_publication_date[1])
)
table(as.character(meta_data[, "publication_date"]))
unlist(meta_publication_date)
length(unlist(meta_publication_date))
length(unlist(meta_publication_date)
)
length(meta_publication_date)
meta_section <- meta(release_corpus, type = "local", tag = "section")
meta_publication_date <- meta(release_corpus, type = "local", tag = "publication_date")
meta_data <- data.frame(
section = unlist(meta_section),
publication_date = meta_publication_date
)
table(as.character(meta_data[, "section"]))
table(as.character(meta_data[, "publication_date"]))
table(as.character(meta_data[, "publication_date"]))
typeof(date)
date
date[1]
dd = date[1]
dd
# Get press release
tmp <- readLines("androidpolice/1.html")
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
release = NULL
release <- xpathSApply(tmp, "//div[@class='post-content']//p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
### GET META DATA:
## https://stackoverflow.com/questions/22342501/how-to-get-information-within-meta-name-tag-in-html-using-htmlparse-and-xpa
# article section (meta data)
# <meta property="article:section" content="Accessories"/>
# meta data: section
content = tmp["//meta/@content"]
# get all meta data, but mark everything except 'property' as NULL
properties = xpathSApply(tmp, "//meta", xmlGetAttr, "property")
# find the meta data containing the 'section' property
sectionIndex = 0;
dateIndex = 0;
for (j in 1:length(properties))
{
if (grepl("article:section",properties[j]) == TRUE)
sectionIndex = j;
if (grepl("article:published_time",properties[j]) == TRUE)
dateIndex = j;
}
# set the section and publication date
section = content[sectionIndex];
date = content[dateIndex];
# make from list to string character
section = paste(c(section), collapse='')
date = paste(c(date), collapse='')
#date = strsplit(date, "T")[[1]] only take the first string (without clock time)
# Create a corpus from a vector
release_corpus = NULL;
release_corpus <- Corpus(VectorSource(release))
head(release_corpus)
# Setting the meta information
meta(release_corpus[[1]], "section") <- section
meta(release_corpus[[1]], "publication_date") <- date[1]
meta(release_corpus[[1]])
n <- 1
## make corpus of all articles
for(i in 2:length(list.files("androidpolice/")))
#for(i in 1:20)
{
tmp <- readLines(str_c("androidpolice/", i, ".html"))
tmp <- str_c(tmp, collapse = "")
tmp <- htmlParse(tmp)
# article content
release <- xpathSApply(tmp, "//div[@class='post-content']//p", xmlValue)
release = str_c(release ,collapse=',')  # make into a single string
# meta data: section
content = tmp["//meta/@content"]
# get all meta data, but mark everything except 'property' as NULL
properties = xpathSApply(tmp, "//meta", xmlGetAttr, "property")
# find the meta data containing the 'section' property
sectionIndex = 0;
dateIndex = 0;
for (j in 1:length(properties))
{
if (grepl("article:section",properties[j]) == TRUE)
sectionIndex = j;
if (grepl("article:published_time",properties[j]) == TRUE)
dateIndex = j;
}
# set the section and publication date
section = content[sectionIndex];
date = content[dateIndex];
# make from list to string character
section = paste(c(section), collapse='')
date = paste(c(date), collapse='')
date = strsplit(date, "T")[[1]] #only take the first string (without clock time)
#print(section)
if(length(release) != 0)
{
n <- n + 1
tmp_corpus <- Corpus(VectorSource(release))
release_corpus <- c(release_corpus, tmp_corpus)
meta(release_corpus[[n]], "section") <- section
meta(release_corpus[[n]], "publication_date") <- date[1]
}
}
release_corpus
# Inspect meta data
meta_section <- meta(release_corpus, type = "local", tag = "section")
meta_publication_date <- meta(release_corpus, type = "local", tag = "publication_date")
meta_data <- data.frame(
section = unlist(meta_section),
publication_date = meta_publication_date
)
table(as.character(meta_data[, "section"]))
table(as.character(meta_data[, "publication_date"]))
meta_data <- data.frame(
section = unlist(meta_section),
publication_date = unlist(meta_publication_date)
)
meta_data <- data.frame(
section = unlist(meta_section),
publication_date = unlist(meta_publication_date)
)
table(as.character(meta_data[, "section"]))
table(as.character(meta_data[, "publication_date"]))
head(meta_data["section"])
head(meta_data["publication_date"])
table(as.character(meta_data[, "section"]))
head(meta_data["section"])
meta_data["section"]
table(as.character(meta_data[, "section"]))
dtm = DocumentTermMatrix(release_corpus);
dtm2 = as.matrix(dtm);
frequency = colSums(dtm2)
frequency = sort(frequency, decreasing=TRUE);
head(frequency) # 6 most common words
frequency = sort(meta_data["section"], decreasing=TRUE);
frequency = sort(as.character(meta_data[, "section"], decreasing=TRUE);
?head
head(meta_data["section"], 100)
table(as.character(meta_data[, "publication_date"]))
table(as.character(meta_data[, "section"]))
meta(release_corpus, tag = "section") == "App Reviews" |
meta(release_corpus, tag = "section") == "Chromecast" |
meta(release_corpus, tag = "section") == "Chromecast" |meta(release_corpus, tag = "section") == "App Reviews"
release_corpus <- release_corpus[
meta(release_corpus, tag = "section") == "Accessories" | #22
meta(release_corpus, tag = "section") == "Android OS" | #10
meta(release_corpus, tag = "section") == "Android TV" | #6
meta(release_corpus, tag = "section") == " Android Wear " | #11
meta(release_corpus, tag = "section") == "APK Teardown " | #6
meta(release_corpus, tag = "section") == "Applications" | #145
meta(release_corpus, tag = "section") == "AT&T" | #11
meta(release_corpus, tag = "section") == "Chromecast" | #5
meta(release_corpus, tag = "section") == "Deals" | #37
meta(release_corpus, tag = "section") == "Development" | #6
meta(release_corpus, tag = "section") == "Device Updates" | #10
meta(release_corpus, tag = "section") == "Games" | #27
meta(release_corpus, tag = "section") == "Google" | #29
meta(release_corpus, tag = "section") == "Leaks" | #6
meta(release_corpus, tag = "section") == "Marshmallow 6.0" | #20
meta(release_corpus, tag = "section") == "News" #54
]
release_corpus
tm_filter(release_corpus, FUN = function(x) any(grep("Nintendo", content(x))))
tm_filter(release_corpus, FUN = function(x) any(grep("Google", content(x))))
tdm <- TermDocumentMatrix(release_corpus)
tdm
release_corpus <- tm_map(release_corpus, removeNumbers)
# Remove punctuation
release_corpus <- tm_map(
release_corpus,
content_transformer(
function(x, pattern){
gsub(
pattern = "[[:punct:]]",
replacement = " ",
x
)
}
)
)
stopwords("en")[1:10]
release_corpus <- tm_map(release_corpus, removeWords, words = stopwords("en"))
release_corpus <- tm_map(release_corpus, content_transformer(tolower))
release_corpus <- tm_map(release_corpus, stemDocument)
tdm <- TermDocumentMatrix(release_corpus)
tdm
tdm <- removeSparseTerms(tdm, 1-(10/length(release_corpus)))
tdm
BigramTokenizer <- function(x){
NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
tdm_bigram <- TermDocumentMatrix(release_corpus, control = list(tokenize = BigramTokenizer))
tdm_bigram
findAssocs(tdm, "Google", .7)
dtm <- DocumentTermMatrix(release_corpus)
dtm <- removeSparseTerms(dtm, 1-(10/length(release_corpus)))
dtm
org_labels <- unlist(meta(release_corpus, "section"))
org_labels[1:3]
org_labels
dtm
N <- length(org_labels)
container <- create_container(
dtm,
labels = org_labels,
trainSize = 1:310,
testSize = 311:N,
virgin = F
)
slotNames(container)
svm_model <- train_model(container, "SVM")
tree_model <- train_model(container, "TREE")
maxent_model <- train_model(container, "MAXENT")
svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)
head(svm_out)
head(tree_out)
head(maxent_out)
labels_out <- data.frame(
correct_label = org_labels[401:N],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F
)
labels_out <- data.frame(
correct_label = org_labels[311:N],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F
)
table(labels_out[,1] == labels_out[,2])
table(labels_out[,1] == labels_out[,3])
table(labels_out[,1] == labels_out[,4])
prop.table(table(labels_out[,1] == labels_out[,4]))
meta_section <- meta(release_corpus, type = "local", tag = "section")
meta_publication_date <- meta(release_corpus, type = "local", tag = "publication_date")
meta_data <- data.frame(
section = unlist(meta_section),
publication_date = unlist(meta_publication_date)
)
table(as.character(meta_data[, "section"]))
table(as.character(meta_data[, "publication_date"]))
head(meta_data["section"])
head(meta_data["publication_date"])
######
# NOT SURE TO USE THIS?
# Filtering the corpus (only those with enought sources; for supervised learning)
release_corpus <- release_corpus[
meta(release_corpus, tag = "section") == "Accessories" | #22
meta(release_corpus, tag = "section") == "Android OS" | #10
#meta(release_corpus, tag = "section") == "Android TV" | #6
meta(release_corpus, tag = "section") == " Android Wear " | #11
#meta(release_corpus, tag = "section") == "APK Teardown " | #6
meta(release_corpus, tag = "section") == "Applications" | #145
meta(release_corpus, tag = "section") == "AT&T" | #11
#meta(release_corpus, tag = "section") == "Chromecast" | #5
meta(release_corpus, tag = "section") == "Deals" | #37
#meta(release_corpus, tag = "section") == "Development" | #6
meta(release_corpus, tag = "section") == "Device Updates" | #10
meta(release_corpus, tag = "section") == "Games" | #27
meta(release_corpus, tag = "section") == "Google" | #29
#meta(release_corpus, tag = "section") == "Leaks" | #6
meta(release_corpus, tag = "section") == "Marshmallow 6.0" | #20
meta(release_corpus, tag = "section") == "News" #54
]
release_corpus
tm_filter(release_corpus, FUN = function(x) any(grep("Google", content(x))))
### 10.2.2 Building a term-document matrix
### --------------------------------------------------------------
# sparse: matrix contains zero's
tdm <- TermDocumentMatrix(release_corpus)
tdm
### 10.2.3 Data cleansing
### --------------------------------------------------------------
# Remove numbers
release_corpus <- tm_map(release_corpus, removeNumbers)
# Remove punctuation
release_corpus <- tm_map(
release_corpus,
content_transformer(
function(x, pattern){
gsub(
pattern = "[[:punct:]]",
replacement = " ",
x
)
}
)
)
# Remove stopwords
length(stopwords("en"))
stopwords("en")[1:10]
release_corpus <- tm_map(release_corpus, removeWords, words = stopwords("en"))
# Convert to lower case
release_corpus <- tm_map(release_corpus, content_transformer(tolower))
# Stem documents
release_corpus <- tm_map(release_corpus, stemDocument)
### 10.2.4 Sparsity and n-grams
### --------------------------------------------------------------
tdm <- TermDocumentMatrix(release_corpus)
tdm
# Remove sparse terms
tdm <- removeSparseTerms(tdm, 1-(10/length(release_corpus)))
tdm
# Bigrams (not needed at the moment...)
BigramTokenizer <- function(x){
NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
tdm_bigram <- TermDocumentMatrix(release_corpus, control = list(tokenize = BigramTokenizer))
tdm_bigram
# Find associations
findAssocs(tdm, "Google", .7)
### 10.3 Supervised Learning Techniques
### --------------------------------------------------------------
### 10.3.5 Application: Governemnt press releases
### --------------------------------------------------------------
dtm <- DocumentTermMatrix(release_corpus)
dtm <- removeSparseTerms(dtm, 1-(10/length(release_corpus)))
dtm
# Labels
org_labels <- unlist(meta(release_corpus, "section"))
org_labels[1:3]
# Create container
N <- length(org_labels)
container <- create_container(
dtm,
labels = org_labels,
trainSize = 1:310,
testSize = 311:N,
virgin = F
)
slotNames(container)
svm_model <- train_model(container, "SVM")
tree_model <- train_model(container, "TREE")
maxent_model <- train_model(container, "MAXENT")
svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)
head(svm_out)
head(tree_out)
head(maxent_out)
labels_out <- data.frame(
correct_label = org_labels[311:N],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F
)
table(labels_out[,1] == labels_out[,2])
table(labels_out[,1] == labels_out[,3])
table(labels_out[,1] == labels_out[,4])
prop.table(table(labels_out[,1] == labels_out[,4]))
meta_data[, "section"])
meta_data[, "section"]
